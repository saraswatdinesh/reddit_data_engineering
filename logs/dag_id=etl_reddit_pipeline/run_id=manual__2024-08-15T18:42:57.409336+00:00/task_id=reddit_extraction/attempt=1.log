[2024-08-15T18:43:00.192+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-08-15T18:43:00.218+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-08-15T18:42:57.409336+00:00 [queued]>
[2024-08-15T18:43:00.226+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-08-15T18:42:57.409336+00:00 [queued]>
[2024-08-15T18:43:00.226+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-08-15T18:43:00.240+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-08-15 18:42:57.409336+00:00
[2024-08-15T18:43:00.260+0000] {standard_task_runner.py:64} INFO - Started process 80 to run task
[2024-08-15T18:43:00.270+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-08-15T18:42:57.409336+00:00', '--job-id', '19', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmpo73epi33']
[2024-08-15T18:43:00.290+0000] {standard_task_runner.py:91} INFO - Job 19: Subtask reddit_extraction
[2024-08-15T18:43:00.584+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-08-15T18:42:57.409336+00:00 [running]> on host 15d5e5353726
[2024-08-15T18:43:00.829+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Dinesh Saraswat' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-08-15T18:42:57.409336+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-08-15T18:42:57.409336+00:00'
[2024-08-15T18:43:00.836+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-08-15T18:43:00.895+0000] {logging_mixin.py:188} INFO - Connected to reddit!
[2024-08-15T18:43:02.035+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': ' As title indicates, I have 10 years of general programming experience and I\'m trying to train myself on the "full" data engineer toolkit. I\'m not looking to rack up thousands of dollars in AWS fees. Does anyone have any recommendations on how to get good at this stuff without spending a ton of money or switching jobs?', 'author_fullname': 't2_7ztprucm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Recently completed Designing Data Intensive Applications - Where should I go from here?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esertq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.95, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 66, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 66, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723674681.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>As title indicates, I have 10 years of general programming experience and I&#39;m trying to train myself on the &quot;full&quot; data engineer toolkit. I&#39;m not looking to rack up thousands of dollars in AWS fees. Does anyone have any recommendations on how to get good at this stuff without spending a ton of money or switching jobs?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1esertq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Square-Mulberry1301'), 'discussion_type': None, 'num_comments': 21, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esertq/recently_completed_designing_data_intensive/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esertq/recently_completed_designing_data_intensive/', 'subreddit_subscribers': 204687, 'created_utc': 1723674681.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.037+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Imagine you have the same requirements as a regular data lake:  \n\n\n* you want to support structured, semi-structured and unstructured data\n* you want to manage the underlying data/files yourself\n* you want sufficient metadata and the ability to tie the data with any query engine\n\nBut there is one big different:\n\n* your scale is miniscule. The data can fit in the RAM of a single machine\n\nWhat stack would you use?', 'author_fullname': 't2_d2c8s0pb5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to build a Small Data Lake? (<100GB)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1est8pl', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.94, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 33, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 33, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723722935.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Imagine you have the same requirements as a regular data lake:  </p>\n\n<ul>\n<li>you want to support structured, semi-structured and unstructured data</li>\n<li>you want to manage the underlying data/files yourself</li>\n<li>you want sufficient metadata and the ability to tie the data with any query engine</li>\n</ul>\n\n<p>But there is one big different:</p>\n\n<ul>\n<li>your scale is miniscule. The data can fit in the RAM of a single machine</li>\n</ul>\n\n<p>What stack would you use?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1est8pl', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='2minutestreaming'), 'discussion_type': None, 'num_comments': 37, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1est8pl/best_way_to_build_a_small_data_lake_100gb/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1est8pl/best_way_to_build_a_small_data_lake_100gb/', 'subreddit_subscribers': 204687, 'created_utc': 1723722935.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.038+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hello everyone!\n\nI’m a Data Analyst at a small health & fitness startup in Mumbai. My day-to-day involves crafting reports to track our sales team's performance and manage our members. I’m now eyeing a shift to Data Engineering and wondering how it might help us(me & my company) to level up our game.\n\nOur data load isn’t exactly enormous, and our tables aren’t overflowing with records. So far, I’ve only thought about automating daily reports with SQLite and similar tools. But is there more I can do? How can Data Engineering take our modest setup and make it shine? I’d love to hear any tips that could boost my skills and give our data domain a little extra pizzazz!", 'author_fullname': 't2_155rsap3j5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Does Data Engineering Matter for Startups?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esqc5z', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 29, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 29, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723711982.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello everyone!</p>\n\n<p>I’m a Data Analyst at a small health &amp; fitness startup in Mumbai. My day-to-day involves crafting reports to track our sales team&#39;s performance and manage our members. I’m now eyeing a shift to Data Engineering and wondering how it might help us(me &amp; my company) to level up our game.</p>\n\n<p>Our data load isn’t exactly enormous, and our tables aren’t overflowing with records. So far, I’ve only thought about automating daily reports with SQLite and similar tools. But is there more I can do? How can Data Engineering take our modest setup and make it shine? I’d love to hear any tips that could boost my skills and give our data domain a little extra pizzazz!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1esqc5z', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No-Sympathy9824'), 'discussion_type': None, 'num_comments': 25, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esqc5z/does_data_engineering_matter_for_startups/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esqc5z/does_data_engineering_matter_for_startups/', 'subreddit_subscribers': 204687, 'created_utc': 1723711982.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.038+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I was wondering what data visualization tools companies that use dbt typically use', 'author_fullname': 't2_4dkbcqg0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'If you’re using dbt for transformations, what data viz tool does your org use?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esszbv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 12, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 12, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723722095.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I was wondering what data visualization tools companies that use dbt typically use</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1esszbv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Rajsuomi'), 'discussion_type': None, 'num_comments': 29, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esszbv/if_youre_using_dbt_for_transformations_what_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esszbv/if_youre_using_dbt_for_transformations_what_data/', 'subreddit_subscribers': 204687, 'created_utc': 1723722095.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.039+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I've done it three times in my career. You start building the infrastructure, ETL, orchestration, data models, BI, and reporting from scratch. Takes about 3-4 years. Then, it all just gets mundane and boring. Then, your manager starts complaining about your performance, despite everything working fantastically and a hundred times better than it ever was. At the beginning, it's fun and exciting, I even look forward to most days! But by the end, nothing but a lot of boredom, and a tremendous amount of anxiety and stress, then eventually I just move on. Why is this the case, and how can I avoid it? ", 'author_fullname': 't2_12aazb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'I get bored once we reach the "mature" stage. Help.', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1et17ur', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 13, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 13, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723743276.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve done it three times in my career. You start building the infrastructure, ETL, orchestration, data models, BI, and reporting from scratch. Takes about 3-4 years. Then, it all just gets mundane and boring. Then, your manager starts complaining about your performance, despite everything working fantastically and a hundred times better than it ever was. At the beginning, it&#39;s fun and exciting, I even look forward to most days! But by the end, nothing but a lot of boredom, and a tremendous amount of anxiety and stress, then eventually I just move on. Why is this the case, and how can I avoid it? </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1et17ur', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ntdoyfanboy'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1et17ur/i_get_bored_once_we_reach_the_mature_stage_help/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1et17ur/i_get_bored_once_we_reach_the_mature_stage_help/', 'subreddit_subscribers': 204687, 'created_utc': 1723743276.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.039+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Dear all, I'm currently a data analyst that creates some databases using Power Platform to be consumed in Power BI.\n\nUsers usually input data through Power Apps or Excel and the data/files are stored in SharePoint libraries and lists.\n\nOne thing that bothers me is that it seems I could do a better job using other tools like Azure SQL, Blob, Functions... But I don't know how to explain this for the stakeholders.\n\nSo, when it's time to move from Power Platform and go to a more robust solution?\n\nP.s.: I work as contractor to a big non tech company. They have the top tier O365 suit and that's why we use Microsoft tools, but with business justification we can have access to other cloud services (I know they also have AWS and GCP contracts).\n\n\nP.s. 2: sorry for my English and not knowing the right technical terms of things. I'm trying to move from data analysis to engineering.", 'author_fullname': 't2_zh4ga', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': "When it's time to upgrade from SharePoint Online to a more elegant database?", 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1eswldu', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 8, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 8, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723732051.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Dear all, I&#39;m currently a data analyst that creates some databases using Power Platform to be consumed in Power BI.</p>\n\n<p>Users usually input data through Power Apps or Excel and the data/files are stored in SharePoint libraries and lists.</p>\n\n<p>One thing that bothers me is that it seems I could do a better job using other tools like Azure SQL, Blob, Functions... But I don&#39;t know how to explain this for the stakeholders.</p>\n\n<p>So, when it&#39;s time to move from Power Platform and go to a more robust solution?</p>\n\n<p>P.s.: I work as contractor to a big non tech company. They have the top tier O365 suit and that&#39;s why we use Microsoft tools, but with business justification we can have access to other cloud services (I know they also have AWS and GCP contracts).</p>\n\n<p>P.s. 2: sorry for my English and not knowing the right technical terms of things. I&#39;m trying to move from data analysis to engineering.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eswldu', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Ceborn'), 'discussion_type': None, 'num_comments': 10, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eswldu/when_its_time_to_upgrade_from_sharepoint_online/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eswldu/when_its_time_to_upgrade_from_sharepoint_online/', 'subreddit_subscribers': 204687, 'created_utc': 1723732051.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.040+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Has anybody run into this and maybe have experience or code--where you have json lines input but the schemas are NOT the same?\n\nProblem part one:\n\nI know spark supports loading json/jsonlines, but there\'s so many distinct key values across all the objects that it overflows and crashes because the schema is so huge, you can\'t load this direct into a df... it doesn\'t even make sense to try to do that in this situation. A df has a schema and these things don\'t have a common schema. They can be made to have a common schema that gets returned back, but we definitely need complex, custom logic applied to each row individually in a UDF to do this.\n\na json line example: {"key": "<insert\\_distinct\\_guid\\_here>": {  "fname":"Bob"  } }\n\nImagine 100 billion of those lines and you\'d have a df with that many cols. Except you don\'t even because spark dies (understandably). The data model is stupid as hell, you should be tarred & feathered for using a GUID for a key. Sadly that\'s beyond my control and I\'m the one who must clean up this mess. This is real DE.\n\nEssentially what I want to do is mask out PII. Recurse through each field where the type is a structure, and scan it for PII fields. I don\'t really care what the key name of the structures are. fname in the above example has a proper key that identifies it as PII but it\'s nested inside that other bullshit.\n\nSo you need to read each line in as text into an rdd, then have a udf munge each row individually\n\nProblem part two:\n\nThis has to be done in scala/java to avoid the notorious pyspark on worker issue. I know it works but the performance hit is epic and my initial crack at this in pyspark (using json library in the udf) ran for over 18hrs before I disqualified it as an option. Has to be scala.\n\nOh and also I\'m doing this in AWS Glue but that shouldn\'t matter that much--I\'m working with only spark stuff way down at the RDD level.', 'author_fullname': 't2_10kwn7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'jsonlines in spark, scala udf', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esib05', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1723687123.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723684162.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Has anybody run into this and maybe have experience or code--where you have json lines input but the schemas are NOT the same?</p>\n\n<p>Problem part one:</p>\n\n<p>I know spark supports loading json/jsonlines, but there&#39;s so many distinct key values across all the objects that it overflows and crashes because the schema is so huge, you can&#39;t load this direct into a df... it doesn&#39;t even make sense to try to do that in this situation. A df has a schema and these things don&#39;t have a common schema. They can be made to have a common schema that gets returned back, but we definitely need complex, custom logic applied to each row individually in a UDF to do this.</p>\n\n<p>a json line example: {&quot;key&quot;: &quot;&lt;insert\\_distinct\\_guid\\_here&gt;&quot;: {  &quot;fname&quot;:&quot;Bob&quot;  } }</p>\n\n<p>Imagine 100 billion of those lines and you&#39;d have a df with that many cols. Except you don&#39;t even because spark dies (understandably). The data model is stupid as hell, you should be tarred &amp; feathered for using a GUID for a key. Sadly that&#39;s beyond my control and I&#39;m the one who must clean up this mess. This is real DE.</p>\n\n<p>Essentially what I want to do is mask out PII. Recurse through each field where the type is a structure, and scan it for PII fields. I don&#39;t really care what the key name of the structures are. fname in the above example has a proper key that identifies it as PII but it&#39;s nested inside that other bullshit.</p>\n\n<p>So you need to read each line in as text into an rdd, then have a udf munge each row individually</p>\n\n<p>Problem part two:</p>\n\n<p>This has to be done in scala/java to avoid the notorious pyspark on worker issue. I know it works but the performance hit is epic and my initial crack at this in pyspark (using json library in the udf) ran for over 18hrs before I disqualified it as an option. Has to be scala.</p>\n\n<p>Oh and also I&#39;m doing this in AWS Glue but that shouldn&#39;t matter that much--I&#39;m working with only spark stuff way down at the RDD level.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esib05', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='bcsamsquanch'), 'discussion_type': None, 'num_comments': 8, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esib05/jsonlines_in_spark_scala_udf/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esib05/jsonlines_in_spark_scala_udf/', 'subreddit_subscribers': 204687, 'created_utc': 1723684162.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.040+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Just curious. My team is going. We all attended the Snowflake Summit earlier this year as well. My manager went to Coalesce last year and said he really enjoyed it. This will be my first time in Vegas so I guess it'll be an experience haha.", 'author_fullname': 't2_556jqozb', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Anyone attending the dbt Coalesce conference this year?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esxs7u', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.79, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723734932.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Just curious. My team is going. We all attended the Snowflake Summit earlier this year as well. My manager went to Coalesce last year and said he really enjoyed it. This will be my first time in Vegas so I guess it&#39;ll be an experience haha.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1esxs7u', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MasterKluch'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esxs7u/anyone_attending_the_dbt_coalesce_conference_this/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esxs7u/anyone_attending_the_dbt_coalesce_conference_this/', 'subreddit_subscribers': 204687, 'created_utc': 1723734932.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.041+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey all, I’m working on a project that has hundreds of people accessing my platform. I started it as a small side project and it has recently blown up. My biggest issue is that there are basic reporting functionalities with filters for different data breakdowns. Before this was sufficient, but now everybody wants something different in terms of data access and charts/exports. Is there a clever way to allow for self-service without exposing all my data from backend to the customer? \n\nIdeas:\n1. Maybe set up a tool like metabase for people to use and query on their own to build reports and dashboards?\n', 'author_fullname': 't2_1h54mq', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Exposing Data for Customers', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esut7g', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1723727807.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723727504.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey all, I’m working on a project that has hundreds of people accessing my platform. I started it as a small side project and it has recently blown up. My biggest issue is that there are basic reporting functionalities with filters for different data breakdowns. Before this was sufficient, but now everybody wants something different in terms of data access and charts/exports. Is there a clever way to allow for self-service without exposing all my data from backend to the customer? </p>\n\n<p>Ideas:\n1. Maybe set up a tool like metabase for people to use and query on their own to build reports and dashboards?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1esut7g', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='onmywaytostealyagirl'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esut7g/exposing_data_for_customers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esut7g/exposing_data_for_customers/', 'subreddit_subscribers': 204687, 'created_utc': 1723727504.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.041+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have a huge dataframe, I have 4 aggregations on same dataframe, but at the end of the aggregations, I generate 4 dataframes with same column set. I want to combine these 4 dataframes having same column set but different row values based aggregations. Currently I am using a union by to combine these datasets. Ofcourse this takes alot of disk space. Can anyone suggest an efficient way to perform this operation.', 'author_fullname': 't2_aw4h40e9c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Efficient replacement for Pyspark Union', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1et0m9n', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1723746117.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723741829.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have a huge dataframe, I have 4 aggregations on same dataframe, but at the end of the aggregations, I generate 4 dataframes with same column set. I want to combine these 4 dataframes having same column set but different row values based aggregations. Currently I am using a union by to combine these datasets. Ofcourse this takes alot of disk space. Can anyone suggest an efficient way to perform this operation.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1et0m9n', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Easy-Experience6270'), 'discussion_type': None, 'num_comments': 12, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1et0m9n/efficient_replacement_for_pyspark_union/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1et0m9n/efficient_replacement_for_pyspark_union/', 'subreddit_subscribers': 204687, 'created_utc': 1723741829.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.042+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi, my background is not in data science, more systems / developer / architect.\n\nI'm currently doing a bit of background reading for a new project I'm working on and I think I'm completly misunderstanding the entire process of data catalog / market place etc.\n\nIn my mind a data eng. / scientist has a data product, they create an entry for this in a data catalog so that everyone else knows it exists.\n\nThe data catalog also deals with taxonomy, allowing data to be categorised so that someone searching through the catalog can find a data product and other similar products.\n\nSo if a user wants to find a data product so that they can use for AI / ML or their BI Tool, they go to the catalog, find it but what is the next step?  \n\nIt seems that some solutions expect all of the data to be replicated from the source systems and held in some sort of central data lake / datasphere /data bricks and you are taken here to actually consume and use the data.\n\nOther data catlogs I have looked at simply have an output port defined which is nothing more than a link to the system that this data is from. \n\nFor some reason I thought the way these platforms would work is that a product is defined along with it's source, this could be a database / api / csv file etc. The platform would have access to this data, automatically pull in metadata and sample dataset, catalog it and catgorise it with similar data, then act as a proxy to allow systems or individual users to pull this data directly from the source without duplicating the data.", 'author_fullname': 't2_63e3rpl', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'explain the final steps after the data catalog please', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esz5fz', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723738224.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi, my background is not in data science, more systems / developer / architect.</p>\n\n<p>I&#39;m currently doing a bit of background reading for a new project I&#39;m working on and I think I&#39;m completly misunderstanding the entire process of data catalog / market place etc.</p>\n\n<p>In my mind a data eng. / scientist has a data product, they create an entry for this in a data catalog so that everyone else knows it exists.</p>\n\n<p>The data catalog also deals with taxonomy, allowing data to be categorised so that someone searching through the catalog can find a data product and other similar products.</p>\n\n<p>So if a user wants to find a data product so that they can use for AI / ML or their BI Tool, they go to the catalog, find it but what is the next step?  </p>\n\n<p>It seems that some solutions expect all of the data to be replicated from the source systems and held in some sort of central data lake / datasphere /data bricks and you are taken here to actually consume and use the data.</p>\n\n<p>Other data catlogs I have looked at simply have an output port defined which is nothing more than a link to the system that this data is from. </p>\n\n<p>For some reason I thought the way these platforms would work is that a product is defined along with it&#39;s source, this could be a database / api / csv file etc. The platform would have access to this data, automatically pull in metadata and sample dataset, catalog it and catgorise it with similar data, then act as a proxy to allow systems or individual users to pull this data directly from the source without duplicating the data.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esz5fz', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='jimbochong'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esz5fz/explain_the_final_steps_after_the_data_catalog/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esz5fz/explain_the_final_steps_after_the_data_catalog/', 'subreddit_subscribers': 204687, 'created_utc': 1723738224.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.042+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I want to create a aws data platform to practice my skill. I have learned alot about some aws data platform architectures and most of aws data services. Are there anything else i can do to improve my skills ?.', 'author_fullname': 't2_8p6qbxiz', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Any idea for AWS data platform project with some other data tools out there intergrated ?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esmgne', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.65, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 4, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 4, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723696775.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I want to create a aws data platform to practice my skill. I have learned alot about some aws data platform architectures and most of aws data services. Are there anything else i can do to improve my skills ?.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1esmgne', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Tuan_coder'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esmgne/any_idea_for_aws_data_platform_project_with_some/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esmgne/any_idea_for_aws_data_platform_project_with_some/', 'subreddit_subscribers': 204687, 'created_utc': 1723696775.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.042+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'https://preview.redd.it/un5etzn59uid1.png?width=1115&format=png&auto=webp&s=53325ac80a8b3d65f597251bc71413eaa26b1c3b\n\nI have created relationship modell based on the starwars api swapi. What I have noticed is that each data set has a many-to-many relationship with each other. So I created some bridge tables.  Starship can have a many-to-many relationship with film but also with people. So I created two separet bridges for respective dataset. The starship that is referencing film is saying what starships did appear in a particular movie. Meanwhile, starship in people dataset is saying what starship did each character use. I hope it makes sense.   \nMy question, is this a good modell or can I improve it somehow?', 'author_fullname': 't2_hzpdyn57x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data modelling recommendation starwars api', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 68, 'top_awarded_type': None, 'hide_score': False, 'media_metadata': {'un5etzn59uid1': {'status': 'valid', 'e': 'Image', 'm': 'image/png', 'p': [{'y': 52, 'x': 108, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=108&crop=smart&auto=webp&s=bbab27f3b5f933ca79852698383d348a9b63f199'}, {'y': 105, 'x': 216, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=216&crop=smart&auto=webp&s=85ed2055be6356043759276f08aafbcdcb8bfa23'}, {'y': 156, 'x': 320, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=320&crop=smart&auto=webp&s=6aeeb52909aedd2e39d08ca1a110fa1df9621ec7'}, {'y': 312, 'x': 640, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=640&crop=smart&auto=webp&s=3ffa082d562cc77ce3c82a0d0550e6e240460664'}, {'y': 468, 'x': 960, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=960&crop=smart&auto=webp&s=52991d2ea383c92a43806e8c5cabdf2afb499a15'}, {'y': 526, 'x': 1080, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=1080&crop=smart&auto=webp&s=5ffd8b77009500be7d0e5d1355f19ae273b0c1f6'}], 's': {'y': 544, 'x': 1115, 'u': 'https://preview.redd.it/un5etzn59uid1.png?width=1115&format=png&auto=webp&s=53325ac80a8b3d65f597251bc71413eaa26b1c3b'}, 'id': 'un5etzn59uid1'}}, 'name': 't3_1eswzyv', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/Z3sHJjkXNHr3EIJ_xSXngljQVKXgkLLBjn7-HE1lnNo.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723733027.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p><a href="https://preview.redd.it/un5etzn59uid1.png?width=1115&amp;format=png&amp;auto=webp&amp;s=53325ac80a8b3d65f597251bc71413eaa26b1c3b">https://preview.redd.it/un5etzn59uid1.png?width=1115&amp;format=png&amp;auto=webp&amp;s=53325ac80a8b3d65f597251bc71413eaa26b1c3b</a></p>\n\n<p>I have created relationship modell based on the starwars api swapi. What I have noticed is that each data set has a many-to-many relationship with each other. So I created some bridge tables.  Starship can have a many-to-many relationship with film but also with people. So I created two separet bridges for respective dataset. The starship that is referencing film is saying what starships did appear in a particular movie. Meanwhile, starship in people dataset is saying what starship did each character use. I hope it makes sense.<br/>\nMy question, is this a good modell or can I improve it somehow?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1eswzyv', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No-Conversation476'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1eswzyv/data_modelling_recommendation_starwars_api/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1eswzyv/data_modelling_recommendation_starwars_api/', 'subreddit_subscribers': 204687, 'created_utc': 1723733027.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.043+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Howdy, I've been working with a few databases at work recently, and we're expanding our reporting capabilities. I've written a few basic python scripts to generate these reports, but we've also been having the databases crash more frequently as a result.\n\nFor some further explanation, the database is a mariaDB with all of the timeseries data stored in BLOB in a simple data_journal table. So anytime I need a specific column from the table, I have to query the data journal and unblob the data. We've had a few instances where tmp partitions on our Linux VMs would fill up and then crash after I had ran ~6 months to a years worth of data requests on it. Has anyone ever had anything like this happen? \n\nThe current vibe/work around is: don't make any large requests of the database, but I don't really see how this is a sustainable solution.", 'author_fullname': 't2_dzk8a', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Mariadb crashes after large queries ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1et1ljx', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723744205.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Howdy, I&#39;ve been working with a few databases at work recently, and we&#39;re expanding our reporting capabilities. I&#39;ve written a few basic python scripts to generate these reports, but we&#39;ve also been having the databases crash more frequently as a result.</p>\n\n<p>For some further explanation, the database is a mariaDB with all of the timeseries data stored in BLOB in a simple data_journal table. So anytime I need a specific column from the table, I have to query the data journal and unblob the data. We&#39;ve had a few instances where tmp partitions on our Linux VMs would fill up and then crash after I had ran ~6 months to a years worth of data requests on it. Has anyone ever had anything like this happen? </p>\n\n<p>The current vibe/work around is: don&#39;t make any large requests of the database, but I don&#39;t really see how this is a sustainable solution.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1et1ljx', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Luemas91'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1et1ljx/mariadb_crashes_after_large_queries/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1et1ljx/mariadb_crashes_after_large_queries/', 'subreddit_subscribers': 204687, 'created_utc': 1723744205.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.043+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi r/dataengineering,\n\nI'm one of the founders of a startup who created a product that can extract data from IT tools and unstructured documents (PDFs, Excels, etc.) and structure it according to pre-defined schemas. Then, we build an index across these data sources. Users can prompt their requests in natural language, LLMs understand the intent, required data sources and generate simple Python scripts that reference the data to generate and deliver custom data sets and superficial analytics.\n\nWe planned to use that to help companies in e-commerce to save them on consolidating fragmented data manually, but increasingly we get approached by technical managers. We identified that data request tickets from non-technical people take a lot off time from data teams, so we thought it might be a good idea to improve our tool so we can simply hand it over to average users, allowing them to do data retrieval themselves.\n\nOur product is still early and we need to  hand-holding, manual data assessments and cleaning processes for each client. But we are getting better at try to figure out where we can bring the most value. That's why I'm asking you:\n\n-- Would it help you if your non-technical co-workers could fetch their data themselves from a tool instead of writing data request tickets?\n\n-- If yes, which tickets drive you mad the most, are the most frequent?\n\n-- Why would you hate our product? :D\n\nThank you a lot for any input, really helping out a few data nerds!", 'author_fullname': 't2_26mmssu0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'What do you think about automating data requests from non-technical people?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esoazf', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.62, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723703621.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi <a href="/r/dataengineering">r/dataengineering</a>,</p>\n\n<p>I&#39;m one of the founders of a startup who created a product that can extract data from IT tools and unstructured documents (PDFs, Excels, etc.) and structure it according to pre-defined schemas. Then, we build an index across these data sources. Users can prompt their requests in natural language, LLMs understand the intent, required data sources and generate simple Python scripts that reference the data to generate and deliver custom data sets and superficial analytics.</p>\n\n<p>We planned to use that to help companies in e-commerce to save them on consolidating fragmented data manually, but increasingly we get approached by technical managers. We identified that data request tickets from non-technical people take a lot off time from data teams, so we thought it might be a good idea to improve our tool so we can simply hand it over to average users, allowing them to do data retrieval themselves.</p>\n\n<p>Our product is still early and we need to  hand-holding, manual data assessments and cleaning processes for each client. But we are getting better at try to figure out where we can bring the most value. That&#39;s why I&#39;m asking you:</p>\n\n<p>-- Would it help you if your non-technical co-workers could fetch their data themselves from a tool instead of writing data request tickets?</p>\n\n<p>-- If yes, which tickets drive you mad the most, are the most frequent?</p>\n\n<p>-- Why would you hate our product? :D</p>\n\n<p>Thank you a lot for any input, really helping out a few data nerds!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esoazf', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='B7456-A5'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esoazf/what_do_you_think_about_automating_data_requests/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esoazf/what_do_you_think_about_automating_data_requests/', 'subreddit_subscribers': 204687, 'created_utc': 1723703621.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.044+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I’m working on building a data pipeline and infrastructure to process ~1M data points daily from multiple sources. My current stack includes Elasticsearch for storage, with pandas and Apache Airflow handling the data processing workflows.\n\nThe main tasks involve sentiment analysis, NER tagging, and potentially other custom AI/ML algorithms. Given the scale and complexity of the data, I’m looking for advice on the following:\n\n1. Optimizing the current pipeline: Are there any best practices I should follow with this setup, especially considering the large volume of data?\n2. Handling model integration: What would be the most efficient way to integrate custom models for sentiment and NER tagging within this pipeline?\n3. Scalability concerns: With data expected to grow further, what are some infrastructure improvements I should consider (e.g., moving to a distributed processing framework, optimizing Elasticsearch usage, etc.)?\n4. Alternative solutions: Should I be exploring other tools or frameworks that might offer better performance or easier management given the nature of the tasks (e.g., Spark, Dask, etc.)?\n\nAny advice, experiences, or resources would be much appreciated! Thanks in advance.', 'author_fullname': 't2_um6h0arzf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Help: Seeking Advice on Optimizing Data Pipeline for Sentiment Analysis, NER Tagging, and Custom AI/ML Workflows', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esmezg', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723696618.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I’m working on building a data pipeline and infrastructure to process ~1M data points daily from multiple sources. My current stack includes Elasticsearch for storage, with pandas and Apache Airflow handling the data processing workflows.</p>\n\n<p>The main tasks involve sentiment analysis, NER tagging, and potentially other custom AI/ML algorithms. Given the scale and complexity of the data, I’m looking for advice on the following:</p>\n\n<ol>\n<li>Optimizing the current pipeline: Are there any best practices I should follow with this setup, especially considering the large volume of data?</li>\n<li>Handling model integration: What would be the most efficient way to integrate custom models for sentiment and NER tagging within this pipeline?</li>\n<li>Scalability concerns: With data expected to grow further, what are some infrastructure improvements I should consider (e.g., moving to a distributed processing framework, optimizing Elasticsearch usage, etc.)?</li>\n<li>Alternative solutions: Should I be exploring other tools or frameworks that might offer better performance or easier management given the nature of the tasks (e.g., Spark, Dask, etc.)?</li>\n</ol>\n\n<p>Any advice, experiences, or resources would be much appreciated! Thanks in advance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esmezg', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Unstable007'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esmezg/help_seeking_advice_on_optimizing_data_pipeline/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esmezg/help_seeking_advice_on_optimizing_data_pipeline/', 'subreddit_subscribers': 204687, 'created_utc': 1723696618.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.044+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I have some background in data science from an internship I did in college, but I chose not to continue with that company because they pollute a lot the environment and some of their practices didn't align with my values.\n\nIn my country, I perceive that data scientist roles seem almost nonexistent. I mostly see job postings for data engineers, data analysts, and business intelligence roles. Choosing between those, I’m leaning towards data engineering because I enjoy coding.\n\nTo build experience, I’m planning to develop some open-source projects using public datasets or even starting from building a database by scraping some websites. And with those datasets, perform common tasks data engineers do. But I’m wondering:\n\n* do companies even care about projects that aren’t done for a business or don’t generate money?\n* Also, do companies actually hire outsiders for junior data engineering roles, or are those positions typically filled by people already in the company?\n\nThe other options are:\n\n* taking any job in a corporation, like data entry jobs or even barely data related jobs and eventually grind between positions to transition into data engineering role\n* starting my own business or startup, maybe lets say in foodservice or mobile gaming, and developing data related tools for it to built experience and showcase value of my data science abilities\n\nI would highly appreciate any guidance, recommendation, experience you could share with me", 'author_fullname': 't2_ufan6avm', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for advice on landing junior data engineer roles', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esekmm', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.54, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1723688649.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723674168.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have some background in data science from an internship I did in college, but I chose not to continue with that company because they pollute a lot the environment and some of their practices didn&#39;t align with my values.</p>\n\n<p>In my country, I perceive that data scientist roles seem almost nonexistent. I mostly see job postings for data engineers, data analysts, and business intelligence roles. Choosing between those, I’m leaning towards data engineering because I enjoy coding.</p>\n\n<p>To build experience, I’m planning to develop some open-source projects using public datasets or even starting from building a database by scraping some websites. And with those datasets, perform common tasks data engineers do. But I’m wondering:</p>\n\n<ul>\n<li>do companies even care about projects that aren’t done for a business or don’t generate money?</li>\n<li>Also, do companies actually hire outsiders for junior data engineering roles, or are those positions typically filled by people already in the company?</li>\n</ul>\n\n<p>The other options are:</p>\n\n<ul>\n<li>taking any job in a corporation, like data entry jobs or even barely data related jobs and eventually grind between positions to transition into data engineering role</li>\n<li>starting my own business or startup, maybe lets say in foodservice or mobile gaming, and developing data related tools for it to built experience and showcase value of my data science abilities</li>\n</ul>\n\n<p>I would highly appreciate any guidance, recommendation, experience you could share with me</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esekmm', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='draktor99'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esekmm/looking_for_advice_on_landing_junior_data/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esekmm/looking_for_advice_on_landing_junior_data/', 'subreddit_subscribers': 204687, 'created_utc': 1723674168.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.044+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "I'm looking for some guidance on how to approach this problem as I do not have a background in data engineering. The problem I'm facing is this: I have multiple airbyte workspaces each with their own data integrations, for the sake of this example they all have the same integrations. Should I:\n\n&nbsp;\n\n1. have a separate databases for each airbyte workspace or\n2. have a separate schema for each airbyte workspace\n\n&nbsp;\n\nThe other thing is how would I go about applying the exact same dbt run transformations to both workspaces in an automated manner? I want to be able to automatically add the transformations to any newly added integrations in new workspaces without.\n\n&nbsp;\n\nWhat makes the most sense to me is organize all the data into a singular database with different schemas applying to  each integration type and then tables within contain ALL the user info for each of those integrations and they are organized by some UUID.", 'author_fullname': 't2_fb4ll', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Snowflake, Airbyte and DBT - Managing multiple workspaces and databases', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esbabh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1723667424.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723665983.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m looking for some guidance on how to approach this problem as I do not have a background in data engineering. The problem I&#39;m facing is this: I have multiple airbyte workspaces each with their own data integrations, for the sake of this example they all have the same integrations. Should I:</p>\n\n<p>&nbsp;</p>\n\n<ol>\n<li>have a separate databases for each airbyte workspace or</li>\n<li>have a separate schema for each airbyte workspace</li>\n</ol>\n\n<p>&nbsp;</p>\n\n<p>The other thing is how would I go about applying the exact same dbt run transformations to both workspaces in an automated manner? I want to be able to automatically add the transformations to any newly added integrations in new workspaces without.</p>\n\n<p>&nbsp;</p>\n\n<p>What makes the most sense to me is organize all the data into a singular database with different schemas applying to  each integration type and then tables within contain ALL the user info for each of those integrations and they are organized by some UUID.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esbabh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='PLxFTW'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esbabh/snowflake_airbyte_and_dbt_managing_multiple/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esbabh/snowflake_airbyte_and_dbt_managing_multiple/', 'subreddit_subscribers': 204687, 'created_utc': 1723665983.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.045+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I have 3 years of experience. I started off with 2 YOE as a data scientist. I then took a DE job for 6 months, and now back at my first employer as a software engineer doing DE sort of stuff. I have an MS in Data Science. \n\nI initially wanted to do DE and enjoyed it for a bit, however it is being “tooled” to death with enterprise software and low code tools across most of industry which I do not enjoy. I am good at it though. \n\n\nOn the flip side, I know just enough DS to be acceptable but lack a strong math/ML background. My DS experience I more analytics focused. \n\n\nI need to buckle down and choose a route. My end goal is to be a Chief Data Officer at a company or startup. My gut says to learn math/ML and go the DS route since it has more exposure business side long term, and I think my DE experience will still be valuable. \n\n\n WWYD?', 'author_fullname': 't2_ruta8e8to', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'On The Fence Between a DS and DE Career at the 3 YOE Mark', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esa0ba', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723662384.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I have 3 years of experience. I started off with 2 YOE as a data scientist. I then took a DE job for 6 months, and now back at my first employer as a software engineer doing DE sort of stuff. I have an MS in Data Science. </p>\n\n<p>I initially wanted to do DE and enjoyed it for a bit, however it is being “tooled” to death with enterprise software and low code tools across most of industry which I do not enjoy. I am good at it though. </p>\n\n<p>On the flip side, I know just enough DS to be acceptable but lack a strong math/ML background. My DS experience I more analytics focused. </p>\n\n<p>I need to buckle down and choose a route. My end goal is to be a Chief Data Officer at a company or startup. My gut says to learn math/ML and go the DS route since it has more exposure business side long term, and I think my DE experience will still be valuable. </p>\n\n<p>WWYD?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1esa0ba', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ParticularBattle2713'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esa0ba/on_the_fence_between_a_ds_and_de_career_at_the_3/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esa0ba/on_the_fence_between_a_ds_and_de_career_at_the_3/', 'subreddit_subscribers': 204687, 'created_utc': 1723662384.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.045+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi there,\n\nI’m capturing realtime data from financial markets and storing it in parquet on S3.\nAs the cheapest structured data storage I’m aware of.\nI’m looking for an efficient process to update this data and avoid duplicates, etc.\n\nI work on Python and looking to make it as cheapest and simple as possible.\n\nI believe this would make sense to consider it as part of the ETL process. So this makes\nme wonder if parquet is a good option for staging.\n\nThanks for you help', 'author_fullname': 't2_clh76j7', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Updating data storage in parquet on S3', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1es8sy8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.63, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Personal Project Showcase', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723659430.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi there,</p>\n\n<p>I’m capturing realtime data from financial markets and storing it in parquet on S3.\nAs the cheapest structured data storage I’m aware of.\nI’m looking for an efficient process to update this data and avoid duplicates, etc.</p>\n\n<p>I work on Python and looking to make it as cheapest and simple as possible.</p>\n\n<p>I believe this would make sense to consider it as part of the ETL process. So this makes\nme wonder if parquet is a good option for staging.</p>\n\n<p>Thanks for you help</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '4134b452-dc3b-11ec-a21a-0262096eec38', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ddbd37', 'id': '1es8sy8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='soyelsimo963'), 'discussion_type': None, 'num_comments': 16, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1es8sy8/updating_data_storage_in_parquet_on_s3/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1es8sy8/updating_data_storage_in_parquet_on_s3/', 'subreddit_subscribers': 204687, 'created_utc': 1723659430.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.046+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Let me know in the comments if I missed anything.', 'author_fullname': 't2_dhgy4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Airbnb Data Tech Stack', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esrc88', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://a.thumbs.redditmedia.com/sFtSCPiCZusKy7fruyhNA5_ERVfvJpujsFH5sPjzDB8.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1723716120.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'junaideffendi.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Let me know in the comments if I missed anything.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.junaideffendi.com/p/airbnb-data-tech-stack?r=cqjft&utm_campaign=post&utm_medium=web', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?auto=webp&s=29f8923c5a9ad9d7c69b7a10fb4d606743736982', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?width=108&crop=smart&auto=webp&s=92c9f8fa80af65bac140e10a807a89f192b3792c', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?width=216&crop=smart&auto=webp&s=b56c1d08e2062415c2eeddcf45c1c306436cadf3', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?width=320&crop=smart&auto=webp&s=7ac25b261efddff8b33ca45fc78649771bd1a3c9', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?width=640&crop=smart&auto=webp&s=6d72b307174b450ec964c8f1d64d30135bbdcd0d', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?width=960&crop=smart&auto=webp&s=5b382fb89e3a2e3be86eb823e7b0edd094144b69', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/1TqNe9CfMNgINoDQqwYds2iFHJ3nL-JowX2GN78LJng.jpg?width=1080&crop=smart&auto=webp&s=90a6c7e1a38098332ff9b1f8fa0e687e22a30215', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'GUtAXHVKVY7Y_F1H3jiY-bM0ZJkWkUmvkkjvJbhh0D8'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1esrc88', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='mjfnd'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esrc88/airbnb_data_tech_stack/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.junaideffendi.com/p/airbnb-data-tech-stack?r=cqjft&utm_campaign=post&utm_medium=web', 'subreddit_subscribers': 204687, 'created_utc': 1723716120.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.046+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hey everyone,\n\nMy friend and I have been toying with the idea of creating a series of open-source data pipelines that could be used by others in the community. The idea is to provide plug-and-play solutions that get you started immediately, integrate with different tech stacks and easy to customize.\n\nWe’re thinking about focusing on a variety of use cases—everything from simple ETL/ELT processes to more complex, industry-specific pipelines (e.g., for finance, healthcare, marketing, sales etc.). The goal would be to save people time and effort for these common use cases.\n\nBefore we dive in, though, we wanted to get a sense of whether this is something that people would find valuable. Specifically:\n\n* **Would you be interested in using open-source data pipelines where you can apply to common data sources?**\xa0For example, we are thinking about creating an ads spending pipeline where we ingest data from various sources, do transformations and aggregations, prepare a list of common queries and create dashboards and reports that can run within your environment (snowflake, databricks, bigquery etc..)\n* **If not? why? What would make an open-source data pipeline more appealing to you (e.g., detailed documentation, strong community support, customization options etc.)?**\n\nWe’d love to hear your thoughts, suggestions, and any potential concerns you might have', 'author_fullname': 't2_14khjrtkjv', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Open source data pipelines', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esocsy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723703828.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone,</p>\n\n<p>My friend and I have been toying with the idea of creating a series of open-source data pipelines that could be used by others in the community. The idea is to provide plug-and-play solutions that get you started immediately, integrate with different tech stacks and easy to customize.</p>\n\n<p>We’re thinking about focusing on a variety of use cases—everything from simple ETL/ELT processes to more complex, industry-specific pipelines (e.g., for finance, healthcare, marketing, sales etc.). The goal would be to save people time and effort for these common use cases.</p>\n\n<p>Before we dive in, though, we wanted to get a sense of whether this is something that people would find valuable. Specifically:</p>\n\n<ul>\n<li><strong>Would you be interested in using open-source data pipelines where you can apply to common data sources?</strong>\xa0For example, we are thinking about creating an ads spending pipeline where we ingest data from various sources, do transformations and aggregations, prepare a list of common queries and create dashboards and reports that can run within your environment (snowflake, databricks, bigquery etc..)</li>\n<li><strong>If not? why? What would make an open-source data pipeline more appealing to you (e.g., detailed documentation, strong community support, customization options etc.)?</strong></li>\n</ul>\n\n<p>We’d love to hear your thoughts, suggestions, and any potential concerns you might have</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1esocsy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Aggressive-Bee-8748'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esocsy/open_source_data_pipelines/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esocsy/open_source_data_pipelines/', 'subreddit_subscribers': 204687, 'created_utc': 1723703828.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.046+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone! I work in a team of purely data analysts and I am one myself (hopefully not for long). We recently deployed a prototype of airflow to automate the flow of some of our data into a central database. Airflow is new to all of us and myself, I've spent the most amount of time on the team having set the pipelines up but my knowledge is limited when it comes to data engineering. \n\nMy background is more that of stats and I have a few models that produce metrics using our data that I would like piped into our database. I believe I can run my models within a dag, that is, do data transformations, model the data, produce predictions & metrics and then pipe those into a db. \n\nMy question is this best way to do this? What other options could I explore, especially since I'd like to able to run more complex models on our data from different data sources, validate them and push those results into other sources. Any advice is appreciated. I guess this is considered MLOps? I have to also admit we're not using airflow the best way but we're hoping to hire a data engineer in the next year or so, but I'd like to have proof of concepts to show management if that's possible. Thank you :)\n", 'author_fullname': 't2_2mb6f1d3', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Machine learning with Airflow', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1esnk9i', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.57, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723700756.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone! I work in a team of purely data analysts and I am one myself (hopefully not for long). We recently deployed a prototype of airflow to automate the flow of some of our data into a central database. Airflow is new to all of us and myself, I&#39;ve spent the most amount of time on the team having set the pipelines up but my knowledge is limited when it comes to data engineering. </p>\n\n<p>My background is more that of stats and I have a few models that produce metrics using our data that I would like piped into our database. I believe I can run my models within a dag, that is, do data transformations, model the data, produce predictions &amp; metrics and then pipe those into a db. </p>\n\n<p>My question is this best way to do this? What other options could I explore, especially since I&#39;d like to able to run more complex models on our data from different data sources, validate them and push those results into other sources. Any advice is appreciated. I guess this is considered MLOps? I have to also admit we&#39;re not using airflow the best way but we&#39;re hoping to hire a data engineer in the next year or so, but I&#39;d like to have proof of concepts to show management if that&#39;s possible. Thank you :)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1esnk9i', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='unmasked_crusader'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1esnk9i/machine_learning_with_airflow/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1esnk9i/machine_learning_with_airflow/', 'subreddit_subscribers': 204687, 'created_utc': 1723700756.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.047+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0xffff6fb23e80>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I am investigating airbyte for a usecase within my company.  the docs seem to imply that all authorization mechanics are only enabled on the coud and enterprise versions.  Is that correct?', 'author_fullname': 't2_13b2ssthzf', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'airbyte authorization?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1es9n7a', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1723661489.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I am investigating airbyte for a usecase within my company.  the docs seem to imply that all authorization mechanics are only enabled on the coud and enterprise versions.  Is that correct?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1es9n7a', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Wise_Tutor892'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1es9n7a/airbyte_authorization/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1es9n7a/airbyte_authorization/', 'subreddit_subscribers': 204687, 'created_utc': 1723661489.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-08-15T18:43:02.047+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-08-15T18:43:02.048+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-08-15T18:43:02.058+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-08-15T18:42:57.409336+00:00, execution_date=20240815T184257, start_date=20240815T184300, end_date=20240815T184302
[2024-08-15T18:43:02.134+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2024-08-15T18:43:02.162+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-08-15T18:43:02.164+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
